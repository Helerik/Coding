{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuation to Pytorch studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is ```torch.nn```\n",
    "\n",
    "PyTorch provides the elegantly designed modules and classes torch.nn , torch.optim , Dataset , and DataLoader to help you create and train neural networks. In order to fully utilize their power and customize them for your problem, you need to really understand exactly what they’re doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19421712d60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAN80lEQVR4nO3df6hcdXrH8c+ncf3DrBpTMYasNhuRWBWbLRqLSl2RrD9QNOqWDVgsBrN/GHChhEr6xyolEuqP0qAsuYu6sWyzLqgYZVkVo6ZFCF5j1JjU1YrdjV6SSozG+KtJnv5xT+Su3vnOzcyZOZP7vF9wmZnzzJnzcLife87Md879OiIEYPL7k6YbANAfhB1IgrADSRB2IAnCDiRxRD83ZpuP/oEeiwiPt7yrI7vtS22/aftt27d281oAesudjrPbniLpd5IWSNou6SVJiyJia2EdjuxAj/XiyD5f0tsR8U5EfCnpV5Ku6uL1APRQN2GfJekPYx5vr5b9EdtLbA/bHu5iWwC61M0HdOOdKnzjND0ihiQNSZzGA03q5si+XdJJYx5/R9L73bUDoFe6CftLkk61/V3bR0r6kaR19bQFoG4dn8ZHxD7bSyU9JWmKpAci4o3aOgNQq46H3jraGO/ZgZ7ryZdqABw+CDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUii4ymbcXiYMmVKsX7sscf2dPtLly5tWTvqqKOK686dO7dYv/nmm4v1u+66q2Vt0aJFxXU///zzYn3lypXF+u23316sN6GrsNt+V9IeSfsl7YuIs+toCkD96jiyXxQRH9TwOgB6iPfsQBLdhj0kPW37ZdtLxnuC7SW2h20Pd7ktAF3o9jT+/Ih43/YJkp6x/V8RsWHsEyJiSNKQJNmOLrcHoENdHdkj4v3qdqekxyTNr6MpAPXrOOy2p9o++uB9ST+QtKWuxgDUq5vT+BmSHrN98HX+PSJ+W0tXk8zJJ59crB955JHF+nnnnVesX3DBBS1r06ZNK6577bXXFutN2r59e7G+atWqYn3hwoUta3v27Cmu++qrrxbrL7zwQrE+iDoOe0S8I+kvauwFQA8x9AYkQdiBJAg7kARhB5Ig7EASjujfl9om6zfo5s2bV6yvX7++WO/1ZaaD6sCBA8X6jTfeWKx/8sknHW97ZGSkWP/www+L9TfffLPjbfdaRHi85RzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtlrMH369GJ948aNxfqcOXPqbKdW7XrfvXt3sX7RRRe1rH355ZfFdbN+/6BbjLMDyRF2IAnCDiRB2IEkCDuQBGEHkiDsQBJM2VyDXbt2FevLli0r1q+44opi/ZVXXinW2/1L5ZLNmzcX6wsWLCjW9+7dW6yfccYZLWu33HJLcV3UiyM7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTB9ewD4JhjjinW200vvHr16pa1xYsXF9e9/vrri/W1a9cW6xg8HV/PbvsB2zttbxmzbLrtZ2y/Vd0eV2ezAOo3kdP4X0i69GvLbpX0bEScKunZ6jGAAdY27BGxQdLXvw96laQ11f01kq6uuS8ANev0u/EzImJEkiJixPYJrZ5oe4mkJR1uB0BNen4hTEQMSRqS+IAOaFKnQ287bM+UpOp2Z30tAeiFTsO+TtIN1f0bJD1eTzsAeqXtabzttZK+L+l429sl/VTSSkm/tr1Y0u8l/bCXTU52H3/8cVfrf/TRRx2ve9NNNxXrDz/8cLHebo51DI62YY+IRS1KF9fcC4Ae4uuyQBKEHUiCsANJEHYgCcIOJMElrpPA1KlTW9aeeOKJ4roXXnhhsX7ZZZcV608//XSxjv5jymYgOcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJx9knulFNOKdY3bdpUrO/evbtYf+6554r14eHhlrX77ruvuG4/fzcnE8bZgeQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtmTW7hwYbH+4IMPFutHH310x9tevnx5sf7QQw8V6yMjIx1vezJjnB1IjrADSRB2IAnCDiRB2IEkCDuQBGEHkmCcHUVnnnlmsX7PPfcU6xdf3Plkv6tXry7WV6xYUay/9957HW/7cNbxOLvtB2zvtL1lzLLbbL9ne3P1c3mdzQKo30RO438h6dJxlv9LRMyrfn5Tb1sA6tY27BGxQdKuPvQCoIe6+YBuqe3XqtP841o9yfYS28O2W/8zMgA912nYfybpFEnzJI1IurvVEyNiKCLOjoizO9wWgBp0FPaI2BER+yPigKSfS5pfb1sA6tZR2G3PHPNwoaQtrZ4LYDC0HWe3vVbS9yUdL2mHpJ9Wj+dJCknvSvpxRLS9uJhx9sln2rRpxfqVV17ZstbuWnl73OHir6xfv75YX7BgQbE+WbUaZz9iAisuGmfx/V13BKCv+LoskARhB5Ig7EAShB1IgrADSXCJKxrzxRdfFOtHHFEeLNq3b1+xfskll7SsPf/888V1D2f8K2kgOcIOJEHYgSQIO5AEYQeSIOxAEoQdSKLtVW/I7ayzzirWr7vuumL9nHPOaVlrN47eztatW4v1DRs2dPX6kw1HdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2SW7u3LnF+tKlS4v1a665plg/8cQTD7mnidq/f3+xPjJS/u/lBw4cqLOdwx5HdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2w0C7sexFi8abaHdUu3H02bNnd9JSLYaHh4v1FStWFOvr1q2rs51Jr+2R3fZJtp+zvc32G7ZvqZZPt/2M7beq2+N63y6ATk3kNH6fpL+PiD+X9FeSbrZ9uqRbJT0bEadKerZ6DGBAtQ17RIxExKbq/h5J2yTNknSVpDXV09ZIurpXTQLo3iG9Z7c9W9L3JG2UNCMiRqTRPwi2T2ixzhJJS7prE0C3Jhx229+W9Iikn0TEx/a4c8d9Q0QMSRqqXoOJHYGGTGjozfa3NBr0X0bEo9XiHbZnVvWZknb2pkUAdWh7ZPfoIfx+Sdsi4p4xpXWSbpC0srp9vCcdTgIzZswo1k8//fRi/d577y3WTzvttEPuqS4bN24s1u+8886WtccfL//KcIlqvSZyGn++pL+V9LrtzdWy5RoN+a9tL5b0e0k/7E2LAOrQNuwR8Z+SWr1Bv7jedgD0Cl+XBZIg7EAShB1IgrADSRB2IAkucZ2g6dOnt6ytXr26uO68efOK9Tlz5nTUUx1efPHFYv3uu+8u1p966qli/bPPPjvkntAbHNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IIk04+znnntusb5s2bJiff78+S1rs2bN6qinunz66acta6tWrSque8cddxTre/fu7agnDB6O7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQRJpx9oULF3ZV78bWrVuL9SeffLJY37dvX7FeuuZ89+7dxXWRB0d2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUjCEVF+gn2SpIcknSjpgKShiPhX27dJuknS/1ZPXR4Rv2nzWuWNAehaRIw76/JEwj5T0syI2GT7aEkvS7pa0t9I+iQi7ppoE4Qd6L1WYZ/I/Owjkkaq+3tsb5PU7L9mAXDIDuk9u+3Zkr4naWO1aKnt12w/YPu4FusssT1se7irTgF0pe1p/FdPtL8t6QVJKyLiUdszJH0gKST9k0ZP9W9s8xqcxgM91vF7dkmy/S1JT0p6KiLuGac+W9KTEXFmm9ch7ECPtQp729N425Z0v6RtY4NefXB30EJJW7ptEkDvTOTT+Ask/Yek1zU69CZJyyUtkjRPo6fx70r6cfVhXum1OLIDPdbVaXxdCDvQex2fxgOYHAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ9HvK5g8k/c+Yx8dXywbRoPY2qH1J9NapOnv7s1aFvl7P/o2N28MRcXZjDRQMam+D2pdEb53qV2+cxgNJEHYgiabDPtTw9ksGtbdB7Uuit071pbdG37MD6J+mj+wA+oSwA0k0Enbbl9p+0/bbtm9toodWbL9r+3Xbm5uen66aQ2+n7S1jlk23/Yztt6rbcefYa6i322y/V+27zbYvb6i3k2w/Z3ub7Tds31Itb3TfFfrqy37r+3t221Mk/U7SAknbJb0kaVFEbO1rIy3YflfS2RHR+BcwbP+1pE8kPXRwai3b/yxpV0SsrP5QHhcR/zAgvd2mQ5zGu0e9tZpm/O/U4L6rc/rzTjRxZJ8v6e2IeCcivpT0K0lXNdDHwIuIDZJ2fW3xVZLWVPfXaPSXpe9a9DYQImIkIjZV9/dIOjjNeKP7rtBXXzQR9lmS/jDm8XYN1nzvIelp2y/bXtJ0M+OYcXCarer2hIb7+bq203j309emGR+YfdfJ9OfdaiLs401NM0jjf+dHxF9KukzSzdXpKibmZ5JO0egcgCOS7m6ymWqa8Uck/SQiPm6yl7HG6asv+62JsG+XdNKYx9+R9H4DfYwrIt6vbndKekyjbzsGyY6DM+hWtzsb7ucrEbEjIvZHxAFJP1eD+66aZvwRSb+MiEerxY3vu/H66td+ayLsL0k61fZ3bR8p6UeS1jXQxzfYnlp9cCLbUyX9QIM3FfU6STdU92+Q9HiDvfyRQZnGu9U042p43zU+/XlE9P1H0uUa/UT+vyX9YxM9tOhrjqRXq583mu5N0lqNntb9n0bPiBZL+lNJz0p6q7qdPkC9/ZtGp/Z+TaPBmtlQbxdo9K3ha5I2Vz+XN73vCn31Zb/xdVkgCb5BByRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ/D+f1mbtgJ8kQQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0].reshape(28,28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = map(torch.tensor, (x_train, y_train, x_valid, y_valid))\n",
    "\n",
    "n, c = x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Layer size\n",
    "l_size = 784\n",
    "\n",
    "# Output size\n",
    "o_size = 10\n",
    "\n",
    "weights = torch.randn(l_size, o_size) / np.sqrt(l_size)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(o_size)\n",
    "bias.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(torch.matmul(xb, weights) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.2573, -2.2865, -1.9381, -2.8032, -2.2473, -2.5310, -2.2618, -2.2792,\n",
      "        -2.5151, -2.1498], grad_fn=<SelectBackward>) torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "xb = x_train[0:batch_size]\n",
    "preds = model(xb)\n",
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb = y_train[0:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4144, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0781)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "l_rate = 0.5\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n-1) // batch_size + 1):\n",
    "        \n",
    "        start_i = i * batch_size\n",
    "        end_i = start_i + batch_size\n",
    "        \n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        \n",
    "        # Forward\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Weight update\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * l_rate\n",
    "            bias -= bias.grad * l_rate\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0793, grad_fn=<NegBackward>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "When using pytorch's ```torch.nn``` module, we are basically skipping most of the steps above that are used when building a neural network. We can see bellow some 'Closing thoughts' that pytorch tutorials provides:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing thoughts\n",
    "\n",
    "We now have a general data pipeline and training loop which you can use for training many types of models using Pytorch. To see how simple training a model can now be, take a look at the mnist_sample sample notebook.\n",
    "\n",
    "Of course, there are many things you’ll want to add, such as data augmentation, hyperparameter tuning, monitoring training, transfer learning, and so forth. These features are available in the fastai library, which has been developed using the same design approach shown in this tutorial, providing a natural next step for practitioners looking to take their models further.\n",
    "\n",
    "We promised at the start of this tutorial we’d explain through example each of torch.nn, torch.optim, Dataset, and DataLoader. So let’s summarize what we’ve seen:\n",
    "\n",
    " - __torch.nn__\n",
    "    - Module: creates a callable which behaves like a function, but can also contain state(such as neural net layer weights). It knows what Parameter (s) it contains and can zero all their gradients, loop through them for weight updates, etc.\n",
    "    - Parameter: a wrapper for a tensor that tells a Module that it has weights that need updating during backprop. Only tensors with the requires_grad attribute set are updated\n",
    "    - functional: a module(usually imported into the F namespace by convention) which contains activation functions, loss functions, etc, as well as non-stateful versions of layers such as convolutional and linear layers.\n",
    " - __torch.optim__: Contains optimizers such as SGD, which update the weights of Parameter during the backward step\n",
    " - __Dataset__: An abstract interface of objects with a ```__len__``` and a ```__getitem__```, including classes provided with Pytorch such as TensorDataset\n",
    " - __DataLoader__: Takes any Dataset and creates an iterator which returns batches of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard: Data and training visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download and import datasets\n",
    "trainset = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 4, shuffle = True, num_workers = 2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size = 4, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes names\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_imshow(img, one_channel = False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim = 0)\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap = \"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = self.pool(F.relu(self.conv1(X)))\n",
    "        X = self.pool(F.relu(self.conv2(X)))\n",
    "        X = X.view(-1, 16 * 4 * 4)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = self.fc3(X)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "NN = NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(NN.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tensorboard setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('runs/fashion_mnist_experiment_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing to Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "my_imshow(img_grid, one_channel = True)\n",
    "\n",
    "writer.add_image('four_fashion_mnist_images', img_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 15848), started 0:00:42 ago. (Use '!kill 15848' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4d478bfe043d4efb\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4d478bfe043d4efb\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Runs tensorboard\n",
    "import os\n",
    "runs_base_dir = \"./runs\"\n",
    "os.makedirs(runs_base_dir, exist_ok=True)\n",
    "%tensorboard --logdir {runs_base_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspect the model using Tensorboard\n",
    "\n",
    "One of TensorBoard’s strengths is its ability to visualize complex model structures. Let’s visualize the model we built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(NN, images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 15848), started 0:00:42 ago. (Use '!kill 15848' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-bec39d04315b8678\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-bec39d04315b8678\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "runs_base_dir = \"./runs\"\n",
    "os.makedirs(runs_base_dir, exist_ok=True)\n",
    "%tensorboard --logdir {runs_base_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Adding a “Projector” to TensorBoard\n",
    "\n",
    "We can visualize the lower dimensional representation of higher dimensional data via the add_embedding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "# helper function\n",
    "def select_n_random(data, labels, n=100):\n",
    "    '''\n",
    "    Selects n random datapoints and their corresponding labels from a dataset\n",
    "    '''\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "# select random images and their target indices\n",
    "images, labels = select_n_random(trainset.data, trainset.targets)\n",
    "\n",
    "# get the class labels for each image\n",
    "class_labels = [classes[lab] for lab in labels]\n",
    "\n",
    "# log embeddings\n",
    "features = images.view(-1, 28 * 28)\n",
    "writer.add_embedding(features,\n",
    "                    metadata=class_labels,\n",
    "                    label_img=images.unsqueeze(1))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 15848), started 0:00:43 ago. (Use '!kill 15848' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8e08000643cfef6\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8e08000643cfef6\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "runs_base_dir = \"./runs\"\n",
    "os.makedirs(runs_base_dir, exist_ok=True)\n",
    "%tensorboard --logdir {runs_base_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tracking model training with tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_probs(network, images):\n",
    "    '''\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    '''\n",
    "    output = network(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classes_preds(network, images, labels):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(network, images)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        my_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            classes[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            classes[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0.\n",
    "for epoch in range(2):\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        inputs, labels = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = NN(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 1000 == 999:\n",
    "            \n",
    "            # Log of running loss\n",
    "            writer.add_scalar('training loss', running_loss / 1000, epoch * len(trainloader) + i)\n",
    "            \n",
    "            # Log figure of training loss\n",
    "            writer.add_figure('predictions vs. actuals', plot_classes_preds(NN, inputs, labels),\n",
    "                             global_step = epoch * len(trainloader) + i)\n",
    "            \n",
    "            running_loss = 0.\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 15848), started 0:10:10 ago. (Use '!kill 15848' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-11429aee934d1851\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-11429aee934d1851\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "runs_base_dir = \"./runs\"\n",
    "os.makedirs(runs_base_dir, exist_ok=True)\n",
    "%tensorboard --logdir {runs_base_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assesing trained models with Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. gets the probability predictions in a test_size x num_classes Tensor\n",
    "# 2. gets the preds in a test_size Tensor\n",
    "# takes ~10 seconds to run\n",
    "class_probs = []\n",
    "class_preds = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        output = NN(images)\n",
    "        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n",
    "        _, class_preds_batch = torch.max(output, 1)\n",
    "\n",
    "        class_probs.append(class_probs_batch)\n",
    "        class_preds.append(class_preds_batch)\n",
    "\n",
    "test_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
    "test_preds = torch.cat(class_preds)\n",
    "\n",
    "# helper function\n",
    "def add_pr_curve_tensorboard(class_index, test_probs, test_preds, global_step=0):\n",
    "    '''\n",
    "    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n",
    "    precision-recall curve\n",
    "    '''\n",
    "    tensorboard_preds = test_preds == class_index\n",
    "    tensorboard_probs = test_probs[:, class_index]\n",
    "\n",
    "    writer.add_pr_curve(classes[class_index],\n",
    "                        tensorboard_preds,\n",
    "                        tensorboard_probs,\n",
    "                        global_step=global_step)\n",
    "    writer.close()\n",
    "\n",
    "# plot all the pr curves\n",
    "for i in range(len(classes)):\n",
    "    add_pr_curve_tensorboard(i, test_probs, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 15848), started 0:18:09 ago. (Use '!kill 15848' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-20cbbcc1e9af8c37\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-20cbbcc1e9af8c37\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "runs_base_dir = \"./runs\"\n",
    "os.makedirs(runs_base_dir, exist_ok=True)\n",
    "%tensorboard --logdir {runs_base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
